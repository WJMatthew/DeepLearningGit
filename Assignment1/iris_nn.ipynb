{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS - torch.nn\n",
    "### M. Johnson, Feb 15, 2019\n",
    "\n",
    "lr = 0.001  \n",
    "batch size = 14\n",
    "\n",
    "Acc: 0.92\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from my_classes import IrisDataset\n",
    "\n",
    "# Data type for tensors\n",
    "dtype = torch.float\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 4])\n",
      "torch.Size([14, 3])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import my_classes as mc\n",
    "\n",
    "##################\n",
    "# IMPORTING DATA #\n",
    "##################\n",
    "\n",
    "iris_data = load_iris()\n",
    "features, pre_labels = iris_data.data, iris_data.target\n",
    "\n",
    "labels = mc.one_hot_encode(pre_labels, 3)\n",
    "\n",
    "feature_train, feature_test, labels_train, labels_test = train_test_split(features, labels, random_state = 17)\n",
    "\n",
    "# Load the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation based on the training data\n",
    "sc.fit(feature_train)\n",
    "\n",
    "# Scale the training data to be of mean 0 and of unit variance\n",
    "feature_train = sc.transform(feature_train)\n",
    "\n",
    "# Scale the test data to be of mean 0 and of unit variance\n",
    "feature_test = sc.transform(feature_test)\n",
    "\n",
    "####################\n",
    "\n",
    "####################\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "batch_size = 14\n",
    "\n",
    "# Training\n",
    "train = data_utils.TensorDataset(torch.from_numpy(feature_train).float().to(device), \n",
    "                                 torch.from_numpy(labels_train).float().to(device))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "data_iter = iter(train_loader)\n",
    "feats, labels = next(data_iter)\n",
    "\n",
    "# Testing\n",
    "test = data_utils.TensorDataset(torch.from_numpy(feature_test).float().to(device), \n",
    "                                 torch.from_numpy(labels_test).float().to(device))\n",
    "test_loader = data_utils.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "print(feats.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)\n",
    "        self.fc2 = nn.Linear(8, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # x = F.softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"IRIS relu h=8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/5000], Loss: 1.057, Acc: 1.0\n",
      "Step [200/5000], Loss: 0.7763, Acc: 1.0\n",
      "Step [300/5000], Loss: 0.6672, Acc: 1.0\n",
      "Step [400/5000], Loss: 0.6345, Acc: 1.0\n",
      "Step [500/5000], Loss: 0.6168, Acc: 1.0\n",
      "Step [600/5000], Loss: 0.5228, Acc: 1.0\n",
      "Step [700/5000], Loss: 0.4604, Acc: 1.0\n",
      "Step [800/5000], Loss: 0.5084, Acc: 1.0\n",
      "Step [900/5000], Loss: 0.4996, Acc: 1.0\n",
      "Step [1000/5000], Loss: 0.3667, Acc: 1.0\n",
      "Step [1100/5000], Loss: 0.5096, Acc: 1.0\n",
      "Step [1200/5000], Loss: 0.56, Acc: 1.0\n",
      "Step [1300/5000], Loss: 0.3704, Acc: 1.0\n",
      "Step [1400/5000], Loss: 0.3796, Acc: 1.0\n",
      "Step [1500/5000], Loss: 0.3998, Acc: 1.0\n",
      "Step [1600/5000], Loss: 0.625, Acc: 1.0\n",
      "Step [1700/5000], Loss: 0.3335, Acc: 1.0\n",
      "Step [1800/5000], Loss: 0.2642, Acc: 1.0\n",
      "Step [1900/5000], Loss: 0.3288, Acc: 1.0\n",
      "Step [2000/5000], Loss: 0.4404, Acc: 1.0\n",
      "Step [2100/5000], Loss: 0.7382, Acc: 1.0\n",
      "Step [2200/5000], Loss: 0.2571, Acc: 1.0\n",
      "Step [2300/5000], Loss: 0.5092, Acc: 1.0\n",
      "Step [2400/5000], Loss: 0.4788, Acc: 1.0\n",
      "Step [2500/5000], Loss: 0.344, Acc: 1.0\n",
      "Step [2600/5000], Loss: 0.4807, Acc: 1.0\n",
      "Step [2700/5000], Loss: 0.4391, Acc: 1.0\n",
      "Step [2800/5000], Loss: 0.3616, Acc: 1.0\n",
      "Step [2900/5000], Loss: 0.5052, Acc: 1.0\n",
      "Step [3000/5000], Loss: 0.3283, Acc: 1.0\n",
      "Step [3100/5000], Loss: 0.6031, Acc: 1.0\n",
      "Step [3200/5000], Loss: 0.4086, Acc: 1.0\n",
      "Step [3300/5000], Loss: 0.2686, Acc: 1.0\n",
      "Step [3400/5000], Loss: 0.6697, Acc: 1.0\n",
      "Step [3500/5000], Loss: 0.4984, Acc: 1.0\n",
      "Step [3600/5000], Loss: 0.2691, Acc: 1.0\n",
      "Step [3700/5000], Loss: 0.2169, Acc: 1.0\n",
      "Step [3800/5000], Loss: 0.4963, Acc: 1.0\n",
      "Step [3900/5000], Loss: 0.4747, Acc: 1.0\n",
      "Step [4000/5000], Loss: 0.4358, Acc: 1.0\n",
      "Step [4100/5000], Loss: 0.2805, Acc: 1.0\n",
      "Step [4200/5000], Loss: 0.2842, Acc: 1.0\n",
      "Step [4300/5000], Loss: 0.4828, Acc: 1.0\n",
      "Step [4400/5000], Loss: 0.4564, Acc: 1.0\n",
      "Step [4500/5000], Loss: 0.1606, Acc: 1.0\n",
      "Step [4600/5000], Loss: 0.2622, Acc: 1.0\n",
      "Step [4700/5000], Loss: 0.3217, Acc: 1.0\n",
      "Step [4800/5000], Loss: 0.426, Acc: 1.0\n",
      "Step [4900/5000], Loss: 0.3991, Acc: 1.0\n",
      "Step [5000/5000], Loss: 0.1862, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "\n",
    "# Model\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "iter_per_epoch = len(train_loader)\n",
    "total_step = 5000\n",
    "\n",
    "loss_list = []\n",
    "loss_dict = {}\n",
    "\n",
    "# Start training\n",
    "for step in range(total_step):\n",
    "\n",
    "    # Reset the data_iter\n",
    "    if (step + 1) % iter_per_epoch == 0:\n",
    "        data_iter = iter(train_loader)\n",
    "\n",
    "    # Grab images and labels\n",
    "    images, labels = next(data_iter)\n",
    "    images.to(device)\n",
    "    labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, torch.max(labels,1)[1])\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_dict[step] = loss\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    # Compute accuracy\n",
    "    _, argmax = torch.max(outputs, 1)\n",
    "    labels_ = torch.max(outputs, 1)[1]\n",
    "    accuracy = (labels_ == argmax.squeeze()).float().mean()\n",
    "\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f'Step [{step+1}/{total_step}], Loss: {loss.item():.4}, Acc: {accuracy.item():.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_loader)):\n",
    "        images, labels = next(test_iter)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        correct += (torch.max(labels, 1)[1] == argmax.squeeze()).float().sum()\n",
    "        total += len(labels)\n",
    "        \n",
    "    \n",
    "    acc = correct/total\n",
    "    print(f'Acc: {acc:.2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
